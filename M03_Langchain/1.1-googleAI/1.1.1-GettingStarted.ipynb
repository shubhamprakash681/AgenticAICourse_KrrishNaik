{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Gemini\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv(\"GOOGLE_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M03_Langchain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-2.5-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000018B7706D6A0> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Generative AI** is a type of artificial intelligence that can **produce new and original content** (such as text, images, audio, video, code, and more) that is realistic and coherent, based on patterns it has learned from a vast amount of existing training data.\\n\\nUnlike traditional AI systems that might classify, predict, or analyze existing data, generative AI\\'s primary function is to **create something novel**.\\n\\nHere\\'s a breakdown of what that means:\\n\\n1.  **Learning Patterns from Data:**\\n    *   Generative AI models are trained on enormous datasets. For example, a text-generating model might read billions of web pages, books, and articles; an image-generating model might analyze millions of pictures.\\n    *   During this training, the model learns the underlying structures, relationships, styles, and characteristics of the data. It doesn\\'t just memorize; it identifies the statistical regularities and rules that govern the content.\\n\\n2.  **Generating New Content:**\\n    *   Once trained, the model can then use this learned knowledge to generate new content that *mimics* the style and characteristics of its training data, but is not a direct copy.\\n    *   Users typically provide a **\"prompt\"** – a specific instruction, description, or starting point – which guides the AI in what to generate.\\n\\n### How it Works (Simplified):\\n\\nImagine you want a generative AI to create a picture of a cat.\\n1.  **Training:** The AI has seen millions of pictures of cats (and other things). It has learned what a cat looks like (ears, whiskers, fur, eyes), how they are typically posed, different breeds, lighting conditions, etc. It understands the \"essence\" of a cat.\\n2.  **Prompt:** You type \"a fluffy orange cat sitting on a windowsill looking at the rain.\"\\n3.  **Generation:** The AI uses its learned patterns to construct an image that matches your description, creating a unique cat picture that has never existed before, but looks perfectly plausible because it adheres to the rules it learned about cats, windowsills, and rain.\\n\\n### Key Characteristics:\\n\\n*   **Creativity and Novelty:** Its core purpose is to produce something new, not just analyze or categorize.\\n*   **Coherence and Realism:** The generated output often looks, sounds, or reads very natural and believable.\\n*   **Prompt-Driven:** Users guide the generation process with specific inputs.\\n*   **Versatility:** Can operate across various \"modalities\" (text, image, audio, etc.).\\n*   **Stochastic Nature:** Often, even with the same prompt, a generative AI can produce slightly different outputs each time, adding to its creative potential.\\n\\n### Common Examples and Applications:\\n\\n*   **Text Generation:** Writing articles, stories, code, emails, summaries, scripts, poetry, marketing copy (e.g., models like GPT-3, GPT-4).\\n*   **Image Generation:** Creating photorealistic images, artwork, logos, product designs, fashion concepts from text descriptions (e.g., DALL-E, Midjourney, Stable Diffusion).\\n*   **Audio Generation:** Composing music, generating speech (text-to-speech), creating sound effects.\\n*   **Video Generation:** Producing short video clips, animating images, generating deepfakes.\\n*   **Code Generation:** Writing programming code, debugging, translating between languages.\\n*   **3D Model Generation:** Creating models for games, simulations, or industrial design.\\n*   **Drug Discovery & Materials Science:** Generating novel molecular structures.\\n\\nGenerative AI represents a significant leap in AI capabilities, making it a powerful tool for creativity, automation, and problem-solving across many industries.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--e3a37888-ed76-4192-bc26-9f84c446ef94-0' usage_metadata={'input_tokens': 6, 'output_tokens': 1803, 'total_tokens': 1809, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1021}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"LangSmith is a **developer platform built by the creators of LangChain** (LangChain, Inc.) that helps developers debug, test, evaluate, and monitor their Language Model (LLM) applications.\\n\\nIn essence, it's an **observability and MLOps platform specifically designed for LLM-powered applications**, which are inherently more complex and non-deterministic than traditional software.\\n\\nHere's a breakdown of what LangSmith is and why it's crucial:\\n\\n---\\n\\n### The Problem LangSmith Solves\\n\\nDeveloping robust LLM applications is challenging due to several factors:\\n\\n1.  **Non-Determinism:** LLMs don't always produce the same output for the same input, making debugging and testing difficult.\\n2.  **Complexity of Chains:** LLM applications often involve multiple steps, API calls, prompt templates, parsing logic, and external tools (RAG, agents). Tracing the flow of data and identifying where issues occur is hard.\\n3.  **Lack of Transparency:** It's often a black box why an LLM responded a certain way, or why a chain failed.\\n4.  **Evaluation Difficulty:** Traditional unit tests aren't sufficient. You need ways to evaluate the quality, accuracy, and performance of LLM outputs.\\n5.  **Production Monitoring:** Once an app is deployed, you need to monitor its performance, cost, latency, and identify regressions or drift.\\n6.  **Iteration Speed:** Experimenting with different prompts, models, and chain architectures requires a systematic way to compare results.\\n\\n---\\n\\n### Key Features of LangSmith\\n\\nLangSmith addresses these challenges with a suite of powerful features:\\n\\n1.  **Tracing & Debugging:**\\n    *   **Visual Execution Flow:** Provides a detailed, step-by-step visualization of your LLM chain's execution. You can see inputs, outputs, intermediate steps, prompt templates, API calls, and tool invocations.\\n    *   **Error Identification:** Easily pinpoint where errors occur within complex chains.\\n    *   **Cost & Latency:** Tracks the cost and latency of each step and the overall chain, helping optimize performance and budget.\\n    *   **Input/Output Inspection:** Examine the exact inputs and outputs at every stage, including raw LLM responses.\\n\\n2.  **Dataset Management & Evaluation:**\\n    *   **Create Datasets:** Build datasets of (input, expected output) pairs or (input, feedback) for systematic testing.\\n    *   **Run Evaluations:** Execute your chains against these datasets.\\n    *   **Automated Evaluators:** Use built-in or custom evaluators (e.g., exact match, semantic similarity, custom LLM-based evaluators) to automatically score your chain's performance.\\n    *   **Human-in-the-Loop Feedback:** Collect human feedback on traces to improve evaluation quality and identify edge cases.\\n    *   **Comparison Views:** Compare the performance of different chain versions, prompts, or models side-by-side using various metrics.\\n\\n3.  **Monitoring & A/B Testing:**\\n    *   **Production Observability:** Collect traces from your live applications to monitor performance, identify regressions, and understand user interactions.\\n    *   **Metric Tracking:** Monitor key metrics like latency, cost, error rate, and custom evaluation scores over time.\\n    *   **A/B Testing:** Deploy different versions of your chains or prompts and compare their real-world performance using live traffic.\\n    *   **Drift Detection:** Identify when your application's behavior or performance starts to degrade.\\n\\n4.  **Prompt Engineering & Versioning:**\\n    *   Manage and version your prompts within LangSmith, allowing for systematic testing and iteration.\\n\\n5.  **Collaboration:**\\n    *   Share traces, datasets, and evaluation results with your team members, fostering a collaborative development environment.\\n\\n---\\n\\n### How it Works (Integration)\\n\\nLangSmith integrates seamlessly with LangChain applications (and can be used with other LLM frameworks, though its deepest integration is with LangChain):\\n\\n1.  **SDK Integration:** You typically enable tracing by setting environment variables (`LANGCHAIN_TRACING_V2=true`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`) in your application.\\n2.  **Automatic Tracing:** When enabled, LangChain automatically sends execution traces to the LangSmith platform.\\n3.  **Web UI:** You interact with these traces, evaluations, and monitoring dashboards through the LangSmith web interface.\\n\\n---\\n\\n### Benefits of Using LangSmith\\n\\n*   **Faster Development Cycle:** Debug and iterate on LLM applications much more quickly.\\n*   **Improved Quality & Reliability:** Build more robust and accurate LLM applications by systematically testing and evaluating changes.\\n*   **Cost Optimization:** Identify and reduce unnecessary LLM calls or expensive steps.\\n*   **Deeper Understanding:** Gain unprecedented transparency into the behavior of your LLM chains.\\n*   **Data-Driven Decision Making:** Make informed decisions about prompt changes, model choices, and architecture improvements based on real data and evaluations.\\n*   **Production Readiness:** Ensure your LLM applications perform well and remain stable in production.\\n\\n---\\n\\nIn summary, LangSmith is an indispensable tool for anyone building serious LLM-powered applications. It transforms the often chaotic process of LLM development into a more structured, observable, and data-driven engineering discipline.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--98171104-684f-42a0-bb8d-616335273082-0' usage_metadata={'input_tokens': 23, 'output_tokens': 2236, 'total_tokens': 2259, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1106}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is an **observability, evaluation, and monitoring platform** specifically designed for **Language Model (LLM) applications**. It's developed by the team behind LangChain, and while it integrates deeply with LangChain, it can also be used with other LLM frameworks.\n",
      "\n",
      "In essence, LangSmith helps developers build, debug, test, and continuously improve their LLM-powered applications by providing unprecedented visibility and control over their behavior.\n",
      "\n",
      "Here's a breakdown of what LangSmith offers:\n",
      "\n",
      "### The Problem LangSmith Solves\n",
      "\n",
      "Building LLM applications is notoriously challenging due to:\n",
      "1.  **Non-determinism:** LLMs don't always give the same output for the same input.\n",
      "2.  **Opacity (\"Black Box\"):** It's hard to understand *why* an LLM produced a particular output, especially in complex chains involving multiple prompts, models, and tools.\n",
      "3.  **Debugging Difficulty:** Traditional debugging tools don't work well for LLM chains.\n",
      "4.  **Evaluation Challenges:** Quantifying the quality of LLM outputs is subjective and complex.\n",
      "5.  **Iterative Development:** Improving LLM apps requires constant experimentation and evaluation.\n",
      "6.  **Production Monitoring:** Identifying regressions or unexpected behavior in production is critical.\n",
      "\n",
      "LangSmith addresses these challenges head-on.\n",
      "\n",
      "### Key Features and Capabilities\n",
      "\n",
      "1.  **Tracing & Observability:**\n",
      "    *   **Visual Debugger:** Provides a visual representation of your LLM application's execution flow (chains, agents, tools, LLM calls). You can see the exact inputs, outputs, intermediate steps, latency, token usage, and errors for every component.\n",
      "    *   **Detailed Call Logs:** Records every interaction, including prompts sent to LLMs, their responses, and the context around those calls.\n",
      "    *   **Reproducibility:** You can easily re-run a specific trace with the same inputs to reproduce bugs or understand behavior.\n",
      "\n",
      "2.  **Debugging:**\n",
      "    *   **Pinpoint Issues:** Quickly identify where errors occur or where the application deviates from expected behavior.\n",
      "    *   **Inspect Prompts & Responses:** See the exact prompts being sent and the raw responses received from the LLM, making prompt engineering much more effective.\n",
      "\n",
      "3.  **Dataset Management:**\n",
      "    *   **Create Datasets from Traces:** Convert interesting or problematic traces into test cases (input/output pairs) for future evaluation.\n",
      "    *   **Upload Custom Datasets:** Import your own ground truth data for benchmarking.\n",
      "    *   **Version Control:** Manage different versions of your datasets.\n",
      "\n",
      "4.  **Evaluation & Benchmarking:**\n",
      "    *   **Automated Evaluators:** Run your LLM application against your datasets using built-in or custom evaluators (e.g., checking for correctness, faithfulness, toxicity, or using an LLM to evaluate another LLM's output).\n",
      "    *   **Human-in-the-Loop Evaluation:** Facilitates human review and feedback on LLM outputs, which is crucial for subjective tasks.\n",
      "    *   **Comparison Runs:** Compare the performance of different versions of your application (e.g., new prompts, different models, updated code) against the same dataset.\n",
      "    *   **Metrics & Dashboards:** Track key performance indicators over time.\n",
      "\n",
      "5.  **Monitoring & Alerting (Production):**\n",
      "    *   **Production Telemetry:** Collect and visualize metrics like latency, cost, error rates, token usage, and user feedback from live applications.\n",
      "    *   **Regression Detection:** Identify when new deployments introduce performance regressions.\n",
      "    *   **Alerting:** Set up alerts for anomalies or critical performance drops.\n",
      "\n",
      "6.  **Collaboration:**\n",
      "    *   Share traces, datasets, and evaluation results with your team, fostering a collaborative development environment.\n",
      "\n",
      "### How it Works\n",
      "\n",
      "You integrate LangSmith into your LLM application (typically by setting an environment variable and potentially wrapping your LLM calls or LangChain components). As your application runs, it sends execution data to the LangSmith cloud service. You then access a web-based UI to view, analyze, and manage this data.\n",
      "\n",
      "### Who is it For?\n",
      "\n",
      "*   **LLM Engineers/Developers:** For debugging, iterating, and improving their applications.\n",
      "*   **Data Scientists:** For evaluating different models and prompt strategies.\n",
      "*   **Product Teams:** For understanding user interactions and identifying areas for improvement.\n",
      "*   **Anyone building production-grade LLM applications.**\n",
      "\n",
      "### Relationship to LangChain\n",
      "\n",
      "LangSmith is developed by the LangChain team and offers the deepest and most seamless integration with the LangChain framework. If you're using LangChain for your LLM application, enabling LangSmith is very straightforward and highly recommended. However, it's also designed to be framework-agnostic, meaning you can use it with other LLM libraries and custom code, though the integration might require a bit more manual instrumentation.\n",
      "\n",
      "In summary, LangSmith transforms LLM application development from a trial-and-error process into a systematic engineering discipline, enabling developers to build more reliable, performant, and robust LLM-powered systems.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M03_Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
