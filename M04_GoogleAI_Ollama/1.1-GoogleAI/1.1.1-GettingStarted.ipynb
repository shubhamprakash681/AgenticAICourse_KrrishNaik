{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Gemini AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv(\"GOOGLE_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-2.5-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001C3A5A2E120> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Generative AI** is a type of artificial intelligence that can **create new and original content** (such as text, images, audio, video, code, and more) that resembles the data it was trained on, but isn\\'t an exact copy.\\n\\nUnlike traditional AI systems that might classify data (e.g., \"is this a cat?\") or make predictions (e.g., \"what will the stock price be?\"), generative AI focuses on **producing novel outputs**.\\n\\nHere\\'s a breakdown of what that means and how it works:\\n\\n1.  **Learning Patterns from Data:**\\n    *   Generative AI models are trained on massive datasets (e.g., billions of text documents, millions of images, hours of audio).\\n    *   During training, they learn the underlying patterns, structures, styles, and relationships within that data. For example, a text-based model learns grammar, syntax, semantics, and common narrative structures. An image model learns how objects look, how light behaves, different artistic styles, etc.\\n\\n2.  **Generating New Content:**\\n    *   Once trained, the model can take a **prompt** (a request or input from a user, often in natural language) and use its learned patterns to *generate* something new that fits the prompt.\\n    *   It\\'s not just retrieving information; it\\'s constructing it piece by piece, predicting the \"next most likely element\" (word, pixel, sound frequency) based on its training and the given prompt.\\n\\n3.  **Key Characteristics:**\\n    *   **Creativity/Novelty:** It can produce outputs that have never existed before.\\n    *   **Versatility:** It can generate content across various modalities (text, image, audio, etc.).\\n    *   **Contextual Understanding:** Many models can understand complex prompts and nuances.\\n    *   **Probabilistic:** The generation process often involves an element of randomness, meaning the same prompt can produce slightly different (but still relevant) results each time.\\n\\n4.  **Common Types of Generative AI and What They Generate:**\\n    *   **Text (Large Language Models - LLMs):**\\n        *   **Examples:** ChatGPT, Google Bard/Gemini, Claude.\\n        *   **Outputs:** Articles, stories, poems, code, emails, summaries, translations, answers to questions, scripts.\\n    *   **Images (Text-to-Image Models):**\\n        *   **Examples:** DALL-E, Midjourney, Stable Diffusion.\\n        *   **Outputs:** Realistic photos, artistic illustrations, concept art, logos, variations of existing images, all from text descriptions.\\n    *   **Audio:**\\n        *   **Examples:** AI music generators, text-to-speech synthesizers.\\n        *   **Outputs:** Music compositions, sound effects, synthetic voices (including voice cloning).\\n    *   **Video:**\\n        *   **Examples:** RunwayML, Google\\'s Lumiere.\\n        *   **Outputs:** Short video clips from text prompts, animating still images, generating new frames.\\n    *   **Code:**\\n        *   **Examples:** GitHub Copilot.\\n        *   **Outputs:** Code snippets, entire functions, debugging suggestions, translating between programming languages.\\n\\n5.  **How it Differs from Traditional AI:**\\n    *   **Traditional AI:** Primarily discriminative (classifies, predicts, identifies patterns in existing data).\\n    *   **Generative AI:** Primarily generative (creates new data that *follows* patterns learned from existing data).\\n\\nGenerative AI represents a significant leap in AI capabilities, enabling powerful new tools for creativity, automation, and problem-solving across many industries. However, it also brings challenges related to ethics, misinformation, copyright, and the potential impact on various professions.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--86f2797d-aba7-4d60-84fa-e3a6ffa4c043-0' usage_metadata={'input_tokens': 6, 'output_tokens': 1965, 'total_tokens': 1971, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1165}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangSmith is a **developer platform for building and monitoring LLM (Large Language Model) applications**, developed by the creators of LangChain. It\\'s designed to help developers debug, test, evaluate, and monitor their LLM-powered applications throughout their entire development lifecycle, from prototyping to production.\\n\\nThink of it as the \"observability and MLOps platform\" specifically tailored for the unique challenges of building with LLMs.\\n\\n## Why LangSmith Exists: The Challenges of LLM Development\\n\\nBuilding robust LLM applications is notoriously difficult due to several factors:\\n\\n1.  **Non-Determinism:** LLMs aren\\'t deterministic. The same prompt can yield different results, making debugging hard.\\n2.  **Black Box Nature:** It\\'s often unclear *why* an LLM produced a certain output.\\n3.  **Chained Components:** Most real-world LLM apps involve multiple steps (retrieval, prompt generation, tool use, multiple LLM calls), making it complex to trace issues.\\n4.  **Evaluation Difficulty:** How do you objectively measure the \"quality\" of an LLM\\'s output? Traditional metrics often don\\'t apply.\\n5.  **Cost & Latency:** LLM calls can be expensive and slow, requiring careful monitoring and optimization.\\n6.  **Prompt Engineering:** Iterating on prompts to get desired behavior is a critical, but often un-tracked, process.\\n\\nLangSmith addresses these challenges by providing tools to gain visibility, control, and systematic improvement over LLM applications.\\n\\n## Key Features and Capabilities of LangSmith\\n\\n1.  **Tracing & Debugging:**\\n    *   **Visual Trace Explorer:** The core feature. It provides a detailed, step-by-step visualization of how your LLM application executes. You can see every LLM call, tool use, intermediate thought, and the inputs/outputs at each stage.\\n    *   **Error Identification:** Easily pinpoint where errors occurred within a complex chain.\\n    *   **Detailed Inspection:** Examine prompts, responses, context, and metadata for each step.\\n\\n2.  **Monitoring & Analytics:**\\n    *   **Key Metrics:** Track essential metrics like latency, token usage, cost, and error rates over time.\\n    *   **Custom Metrics:** Define and track metrics specific to your application\\'s success criteria (e.g., number of relevant chunks retrieved, sentiment of response).\\n    *   **User Feedback:** Integrate mechanisms to collect user feedback directly on responses, which can then be used for evaluation and improvement.\\n\\n3.  **Dataset & Evaluation:**\\n    *   **Dataset Creation:** Create and manage test datasets (input/output pairs) for your LLM applications.\\n    *   **Automated Evaluation:** Run automated evaluations against your datasets using various methods:\\n        *   **LLM-as-a-judge:** Use another LLM to score the quality of responses.\\n        *   **Heuristic-based:** Define rules or regular expressions to check for certain patterns.\\n        *   **RAG-specific metrics:** Evaluate retrieval quality (e.g., faithfulness, answer relevance).\\n    *   **Human-in-the-Loop Evaluation:** Facilitate human review and annotation of responses.\\n    *   **Comparison:** Compare the performance of different models, prompt versions, or chain configurations against your datasets.\\n\\n4.  **Prompt Management:**\\n    *   Store, version, and manage your prompts centrally.\\n    *   Experiment with different prompt templates and compare their performance.\\n\\n5.  **Playground:**\\n    *   An interactive environment to quickly test and iterate on prompts and small chains without deploying your full application.\\n\\n6.  **Collaboration:**\\n    *   Share traces, datasets, and evaluation results with team members.\\n\\n## How it Relates to LangChain\\n\\nLangSmith is developed by the same team as LangChain, and they are designed to work together seamlessly:\\n\\n*   **LangChain (Open-Source Framework):** Provides the building blocks and abstractions (chains, agents, tools, retrievers) for constructing LLM applications.\\n*   **LangSmith (SaaS Platform):** Provides the observability, debugging, testing, and evaluation capabilities *for* those applications (and others).\\n\\nWhile LangSmith has deep integrations with LangChain, it\\'s important to note:\\n*   You can use **LangSmith to monitor applications *not built with LangChain*** (e.g., pure OpenAI API calls, LlamaIndex, etc.), as long as you integrate its SDK.\\n*   You can use **LangChain to build applications *without* using LangSmith**, though you\\'ll miss out on the dedicated observability tools.\\n\\nThey are complementary, with LangSmith significantly enhancing the development experience for LangChain users.\\n\\n## Who is LangSmith For?\\n\\n*   **LLM Developers:** For debugging complex chains and understanding application behavior.\\n*   **ML Engineers:** For monitoring performance in production and identifying areas for optimization.\\n*   **Data Scientists:** For creating and evaluating datasets to improve model quality.\\n*   **Product Managers:** For understanding application performance and user satisfaction.\\n*   **Teams building any generative AI application:** From simple chatbots to complex autonomous agents, LangSmith helps ensure quality and reliability.\\n\\nIn essence, LangSmith bridges the gap between prototyping LLM applications and deploying them confidently into production, enabling data-driven iteration and continuous improvement.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--30d538fb-15e1-4bcc-827f-72a4f95d08c3-0' usage_metadata={'input_tokens': 23, 'output_tokens': 2368, 'total_tokens': 2391, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1238}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a **developer platform for building, debugging, testing, and monitoring Large Language Model (LLM) applications**, created by the team behind LangChain.\n",
      "\n",
      "In essence, it provides the necessary tooling to move LLM applications from experimental prototypes to reliable, production-ready systems.\n",
      "\n",
      "## Why is LangSmith Needed? The Challenges of LLM Development\n",
      "\n",
      "Developing robust LLM applications comes with unique challenges that traditional software development tools aren't well-equipped to handle:\n",
      "\n",
      "1.  **Nondeterminism:** LLMs are inherently probabilistic. The same input might yield slightly different outputs, making debugging difficult.\n",
      "2.  **Complexity of Chains/Agents:** LLM applications often involve multiple steps: retrieving data (RAG), calling tools, making multiple LLM calls, parsing outputs, etc. Understanding the flow and identifying where things go wrong is hard.\n",
      "3.  **Observability Gap:** It's difficult to \"see inside\" an LLM call or a complex chain to understand *why* it produced a certain output or failed.\n",
      "4.  **Evaluation:** How do you objectively measure the performance of an LLM application? Traditional unit tests are often insufficient.\n",
      "5.  **Iteration Speed:** Experimenting with different prompts, models, and architectures requires rapid testing and comparison.\n",
      "6.  **Production Monitoring:** Once deployed, how do you track performance, latency, cost, and user satisfaction?\n",
      "\n",
      "LangSmith addresses these challenges by providing **observability, evaluation, and monitoring capabilities** specifically designed for the LLM stack.\n",
      "\n",
      "## Key Features and Capabilities of LangSmith\n",
      "\n",
      "LangSmith can be broadly categorized into three core pillars:\n",
      "\n",
      "### 1. Observability & Debugging\n",
      "\n",
      "This is often the first point of entry for most users.\n",
      "\n",
      "*   **Traces (Runs):** LangSmith records every \"run\" or \"trace\" of your LLM application. A trace is a detailed, hierarchical log of all operations:\n",
      "    *   Inputs and outputs of the entire application.\n",
      "    *   Individual LLM calls (prompts, responses, tokens used, latency, cost).\n",
      "    *   Calls to tools, retrievers, and other components.\n",
      "    *   Intermediate steps within a LangChain chain or agent.\n",
      "*   **Visual Debugger:** It provides a rich UI to visualize these traces as a tree structure. This allows developers to:\n",
      "    *   Step through the execution flow.\n",
      "    *   Inspect inputs and outputs at each step.\n",
      "    *   Identify where an error occurred or where an unexpected response originated.\n",
      "    *   Compare different runs side-by-side.\n",
      "*   **Filtering & Search:** Easily search and filter traces based on time, status (success/failure), components involved, user ID, and custom tags.\n",
      "\n",
      "### 2. Testing & Evaluation\n",
      "\n",
      "Moving beyond simple debugging, LangSmith helps you systematically test and evaluate your LLM applications.\n",
      "\n",
      "*   **Datasets:** You can create and manage datasets of inputs, and optionally, ground truth outputs. These datasets can be:\n",
      "    *   Curated manually.\n",
      "    *   Extracted from production traces (e.g., \"this user query caused a bad response, let's add it to our test set\").\n",
      "*   **Evaluators:** LangSmith supports various ways to evaluate runs against your datasets:\n",
      "    *   **Automated Evaluators:** Built-in metrics (e.g., latency, token count), or LLM-as-a-judge evaluators (where an LLM assesses the quality of another LLM's output).\n",
      "    *   **Human Feedback:** Integrate mechanisms to collect human feedback (e.g., thumbs up/down, specific ratings) on outputs, both during development and in production.\n",
      "*   **Comparison View:** Easily compare the performance of different versions of your application (e.g., a new prompt vs. an old one, or two different LLM models) across your datasets using various metrics. This is crucial for A/B testing and iterative improvement.\n",
      "\n",
      "### 3. Monitoring & Production Management\n",
      "\n",
      "Once your application is deployed, LangSmith helps you keep an eye on its performance.\n",
      "\n",
      "*   **Production Monitoring:** Track key metrics in real-time:\n",
      "    *   Latency (overall and per component).\n",
      "    *   Cost (token usage, API calls).\n",
      "    *   Error rates.\n",
      "    *   Throughput.\n",
      "    *   User feedback trends.\n",
      "*   **Feedback Loops:** Collect user feedback directly from your deployed application and use it to:\n",
      "    *   Identify areas for improvement.\n",
      "    *   Curate new test cases for your evaluation datasets.\n",
      "    *   Trigger alerts for critical issues.\n",
      "*   **A/B Testing in Production:** Deploy different versions of your LLM application to different user segments and use LangSmith to compare their performance and user satisfaction metrics.\n",
      "*   **Prompt & Model Versioning:** Keep track of different prompt templates and model versions used, and see how they correlate with performance.\n",
      "\n",
      "## How it Integrates\n",
      "\n",
      "LangSmith is deeply integrated with **LangChain** (both Python and JavaScript/TypeScript SDKs). By simply setting a few environment variables (`LANGCHAIN_TRACING_V2=true`, `LANGCHAIN_API_KEY`, `LANGCHAIN_PROJECT`), your LangChain applications will automatically send traces to LangSmith.\n",
      "\n",
      "While it has native integration with LangChain, you can also use LangSmith to trace and evaluate applications built with other frameworks (like LlamaIndex, OpenAI's SDK directly, etc.) by using its client library for custom instrumentation.\n",
      "\n",
      "## Who Uses LangSmith?\n",
      "\n",
      "*   **LLM Application Developers:** For debugging complex chains, understanding LLM behavior, and ensuring reliability.\n",
      "*   **Prompt Engineers:** For experimenting with and iterating on prompts, and evaluating their effectiveness.\n",
      "*   **ML Engineers / MLOps Teams:** For monitoring LLM applications in production, collecting feedback, and managing deployments.\n",
      "*   **Teams building RAG systems, AI agents, chatbots, content generation tools, etc.**\n",
      "\n",
      "In summary, LangSmith is an essential tool for anyone serious about building, refining, and maintaining high-quality, production-grade LLM applications. It provides the visibility and control necessary to navigate the unique complexities of the LLM development lifecycle.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M04_GoogleAI_Ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
