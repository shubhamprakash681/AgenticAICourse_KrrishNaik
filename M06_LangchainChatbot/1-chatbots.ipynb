{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "In this video We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This video tutorial will cover the basics which will be helpful for those two more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000014DEC149A90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014DEC14A660>, model_name='openai/gpt-oss-120b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"openai/gpt-oss-120b\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Shubham! 👋 Great to meet a Chief AI Engineer. How can I assist you today? Whether you’d like to discuss a technical challenge, explore the latest AI research, brainstorm product ideas, or anything else, I’m here to help.', additional_kwargs={'reasoning_content': \"We need to respond. The user introduced themselves. Probably we should greet them, ask how can help. Since they're a Chief AI Engineer, maybe discuss AI topics. Follow guidelines: be friendly, ask how can assist.\"}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 86, 'total_tokens': 192, 'completion_time': 0.21049726, 'prompt_time': 0.002980569, 'queue_time': 0.051540491, 'total_time': 0.213477829}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ed9190d8b7', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--0e7aa2bd-9f1b-4782-bcf7-0251c2339b82-0', usage_metadata={'input_tokens': 86, 'output_tokens': 106, 'total_tokens': 192})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Shubham and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Shubham**, and you’re a **Chief AI Engineer**. How can I help you today?', additional_kwargs={'reasoning_content': 'The user asks: \"Hey What\\'s my name and what do I do?\" The user earlier said: \"Hi , My name is Shubham and I am a Chief AI Engineer\". So we respond with that. Also possibly ask if they\\'d like anything else. Should be friendly.'}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 142, 'total_tokens': 233, 'completion_time': 0.180643297, 'prompt_time': 0.005725, 'queue_time': 0.04747919, 'total_time': 0.186368297}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ed9190d8b7', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--205ea222-8908-4a7d-85f3-a3e3a9f91538-0', usage_metadata={'input_tokens': 142, 'output_tokens': 91, 'total_tokens': 233})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Shubham and I am a Chief AI Engineer\"),\n",
    "        AIMessage(content=\"Hello Shubham! 👋 It’s great to meet you. As a Chief AI Engineer, you must be working on some fascinating projects. How can I assist you today?\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Shubham and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Shubham! 👋 It’s great to meet a Chief AI Engineer. How can I assist you today? Whether you have technical questions, need brainstorming ideas, or just want to chat about the latest AI trends, I’m here to help.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is **Shubham**.', additional_kwargs={'reasoning_content': 'The user asks \"What\\'s my name?\" The user previously said \"Hi, My name is Shubham and I am a Chief AI Engineer\". So answer: Shubham.'}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 151, 'total_tokens': 206, 'completion_time': 0.109279765, 'prompt_time': 0.005743172, 'queue_time': 0.050895898, 'total_time': 0.115022937}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--045dc8bf-4ef0-4a3d-bdfa-adb692211581-0', usage_metadata={'input_tokens': 151, 'output_tokens': 55, 'total_tokens': 206})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m sorry—I don’t have any information about your name. If you’d like, you can tell me what you’d like to be called, and I’ll use that going forward!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change the config-->session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, John! How can I help you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is **John**.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer all the questions to the best of your ability.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Shubham! 👋 Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'We need to respond as ChatGPT. The user says \"Hi My name is Shubham\". We can greet back. Possibly ask how can help. Use friendly tone.'}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 98, 'total_tokens': 162, 'completion_time': 0.130236606, 'prompt_time': 0.003449528, 'queue_time': 0.050902902, 'total_time': 0.133686134}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_3a688838c3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--fc817750-019b-46a7-8e03-0a54d1c0c0c5-0', usage_metadata={'input_tokens': 98, 'output_tokens': 64, 'total_tokens': 162})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Shubham\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Shubham! Nice to meet you. How can I assist you today?', additional_kwargs={'reasoning_content': 'The user says \"Hi My name is Shubham\". Probably a greeting and introduction. We should respond politely, maybe ask how can I help. Use friendly tone.'}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 98, 'total_tokens': 159, 'completion_time': 0.12280823, 'prompt_time': 0.003637098, 'queue_time': 0.051584141, 'total_time': 0.126445328}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ed9190d8b7', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--72042cb9-ad19-496e-b615-1f857cfb1c7d-0', usage_metadata={'input_tokens': 98, 'output_tokens': 61, 'total_tokens': 159})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Shubham\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Shubham.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते शु्भम! आपसे मिलकर खुशी हुई। आज मैं आपकी किस प्रकार मदद कर सकता हूँ?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Shubham\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नमस्ते शुभम! आपसे मिलकर ख़ुशी हुई। आप कैसे हैं?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am Shubham\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आपका नाम **शुभम** है।'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      2\u001b[39m trimmer=trim_messages(\n\u001b[32m      3\u001b[39m     max_tokens=\u001b[32m45\u001b[39m,\n\u001b[32m      4\u001b[39m     strategy=\u001b[33m\"\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     start_on=\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m messages = [\n\u001b[32m     11\u001b[39m     SystemMessage(content=\u001b[33m\"\u001b[39m\u001b[33myou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre a good assistant\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m     HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mhi! I\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm Shubham\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     AIMessage(content=\u001b[33m\"\u001b[39m\u001b[33myes!\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     22\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtrimmer\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5024\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this ``Runnable`` synchronously.\u001b[39;00m\n\u001b[32m   5010\u001b[39m \n\u001b[32m   5011\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5021\u001b[39m \n\u001b[32m   5022\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m5024\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5025\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5026\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5028\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5030\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5031\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2089\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2085\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2086\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2087\u001b[39m         output = cast(\n\u001b[32m   2088\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2089\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2090\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2091\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2092\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2093\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2094\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2095\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2096\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2097\u001b[39m         )\n\u001b[32m   2098\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2099\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4881\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4879\u001b[39m                 output = chunk\n\u001b[32m   4880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4881\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4882\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4883\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4884\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4885\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:430\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    429\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:1015\u001b[39m, in \u001b[36mtrim_messages\u001b[39m\u001b[34m(messages, max_tokens, token_counter, strategy, allow_partial, end_on, start_on, include_system, text_splitter)\u001b[39m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _first_max_tokens(\n\u001b[32m   1007\u001b[39m         messages,\n\u001b[32m   1008\u001b[39m         max_tokens=max_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m         end_on=end_on,\n\u001b[32m   1013\u001b[39m     )\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strategy == \u001b[33m\"\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1015\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_max_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_counter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_token_counter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_system\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_system\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_splitter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m. Supported strategies are \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:1561\u001b[39m, in \u001b[36m_last_max_tokens\u001b[39m\u001b[34m(messages, max_tokens, token_counter, text_splitter, allow_partial, include_system, start_on, end_on)\u001b[39m\n\u001b[32m   1559\u001b[39m remaining_tokens = max_tokens\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m system_message:\n\u001b[32m-> \u001b[39m\u001b[32m1561\u001b[39m     system_tokens = \u001b[43mtoken_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1562\u001b[39m     remaining_tokens = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, max_tokens - system_tokens)\n\u001b[32m   1564\u001b[39m reversed_result = _first_max_tokens(\n\u001b[32m   1565\u001b[39m     reversed_messages,\n\u001b[32m   1566\u001b[39m     max_tokens=remaining_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1570\u001b[39m     end_on=start_on,\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:417\u001b[39m, in \u001b[36mBaseLanguageModel.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    413\u001b[39m     warnings.warn(\n\u001b[32m    414\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    415\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    416\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:417\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    413\u001b[39m     warnings.warn(\n\u001b[32m    414\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    415\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    416\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:388\u001b[39m, in \u001b[36mBaseLanguageModel.get_num_tokens\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    377\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n\u001b[32m    378\u001b[39m \n\u001b[32m    379\u001b[39m \u001b[33;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    386\u001b[39m \n\u001b[32m    387\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:374\u001b[39m, in \u001b[36mBaseLanguageModel.get_token_ids\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_get_token_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_get_token_ids(text)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:95\u001b[39m, in \u001b[36m_get_token_ids_default_method\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m tokenizer = \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.encode(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shubham Prakash\\11Drive\\myCodes\\AI\\AgenticAI_KrishNaik\\M06_LangchainChatbot\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\base.py:87\u001b[39m, in \u001b[36mget_tokenizer\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_TRANSFORMERS:\n\u001b[32m     82\u001b[39m     msg = (\n\u001b[32m     83\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m GPT2TokenizerFast.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`."
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm Shubham\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to your personal preferences like your favorite ice cream flavor.  \\n\\nWhat's your favorite ice cream? 😊🍦\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    "    \n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"whats 2 + 2\" 😊  \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets wrap this in the MEssage History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't have access to past conversations or any personal information about you, including your name.  \\n\\nIf you'd like to tell me your name, I'd be happy to know! 😊  \\n\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I have no memory of past conversations. If you'd like to ask me a math problem, I'm happy to help! 😊  Just let me know what it is. \\n\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M06_LangchainChatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
